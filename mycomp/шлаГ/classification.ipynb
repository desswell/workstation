{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34844b79-b9cf-4cba-9163-74e34f64954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c9c2076-9645-4511-85b3-1dbd56a9cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import os\n",
    "import torch\n",
    "import docx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc33590a-1c0b-4b6f-a98f-798d915e74c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'OpenBuddy/openbuddy-mixtral-7bx8-v18.1-32k'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa5f166-2ecf-452e-97fe-e04dab053dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe8b69b-67e4-4921-a80d-e439c5e02506",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    use_flash_attention_2=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a14d9a-c888-4c95-86f7-cccf0425e87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103d9c954eab46f4b1c5f4d9599e213a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19558c2d-4b98-4b6a-87e4-8efb96edc645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    topics = {}\n",
    "    current_topic = \"\"\n",
    "    current_subtopic = \"\"\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip():\n",
    "            if para.text[0].isdigit() and '.' in para.text:  # Subtopic\n",
    "                current_subtopic = para.text.strip()\n",
    "                topics[current_subtopic] = {\n",
    "                    \"keywords_all\": [],\n",
    "                    \"keywords_any\": [],\n",
    "                    \"exclude\": []\n",
    "                }\n",
    "            elif para.text.startswith(\"Ключевые слова одновременно необходимые для поиска (и):\"):\n",
    "                keywords = para.text.replace(\"Ключевые слова одновременно необходимые для поиска (и):\", \"\").strip().split(';')\n",
    "                topics[current_subtopic][\"keywords_all\"].extend([k.strip() for k in keywords])\n",
    "            elif para.text.startswith(\"Ключевые слова при поиске которых достаточно совпадения одного слова или словосочетания (или):\"):\n",
    "                keywords = para.text.replace(\"Ключевые слова при поиске которых достаточно совпадения одного слова или словосочетания (или):\", \"\").strip().split(';')\n",
    "                topics[current_subtopic][\"keywords_any\"].extend([k.strip() for k in keywords])\n",
    "            elif para.text.startswith(\"Исключить слова:\"):\n",
    "                exclude_words = para.text.replace(\"Исключить слова:\", \"\").strip().split(';')\n",
    "                topics[current_subtopic][\"exclude\"].extend([e.strip() for e in exclude_words])\n",
    "    return topics\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        embeddings = outputs.hidden_states[-1][:,0,:].squeeze().cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "def classify_document(content, topics):\n",
    "    content_embedding = get_embedding(content)\n",
    "    max_similarity = -1\n",
    "    best_topic = \"Unclassified\"\n",
    "\n",
    "    for topic, rules in topics.items():\n",
    "        topic_keywords = \" \".join(rules[\"keywords\"])\n",
    "        topic_embedding = get_embedding(topic_keywords)\n",
    "        similarity = cosine_similarity([content_embedding], [topic_embedding])[0][0]\n",
    "        \n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_topic = topic\n",
    "\n",
    "    return best_topic\n",
    "\n",
    "def sanitize_folder_name(name):\n",
    "    # Удаляем или заменяем недопустимые символы\n",
    "    invalid_chars = '<>:\"/\\\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        name = name.replace(char, '')\n",
    "    return name\n",
    "\n",
    "def distribute_documents(doc_folder, docx_file):\n",
    "    topics = read_docx(docx_file)\n",
    "    unclassified_folder = os.path.join(doc_folder, 'unclassified')\n",
    "    if not os.path.exists(unclassified_folder):\n",
    "        os.makedirs(unclassified_folder)\n",
    "    for root, dirs, files in os.walk(doc_folder):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                content = read_txt(file_path)\n",
    "                topic = classify_document(content, topics)\n",
    "                sanitized_topic = sanitize_folder_name(topic)\n",
    "                topic_folder = os.path.join(doc_folder, sanitized_topic)\n",
    "                if not os.path.exists(topic_folder):\n",
    "                    os.makedirs(topic_folder)\n",
    "                new_file_path = os.path.join(topic_folder, filename) if topic != \"Unclassified\" else os.path.join(unclassified_folder, filename)\n",
    "                os.rename(file_path, os.path.join(topic_folder, new_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e81c623e-198c-4d59-bd5c-1e9c323f648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ФУНКИЯ НАПИСАНА ДЛЯ ПРОДОЛЖЕНИЯ КЛАССИФИКАЦИИ, В СЛУЧАЕ ЕСЛИ МЫ ПРЕРВАЛИ ЕЕ\n",
    "def distribute_documents(doc_folder, docx_file):\n",
    "    topics = read_docx(docx_file)\n",
    "    folder_pattern = re.compile(r'^\\d{4}_\\d{2}$')\n",
    "    for folder in os.listdir(doc_folder):\n",
    "        folder_path = os.path.join(doc_folder, folder)\n",
    "        if os.path.isdir(folder_path) and folder_pattern.match(folder):  # Проверка соответствия формату NNNN_NN и является ли это папкой\n",
    "            for root, _, files in os.walk(folder_path):\n",
    "                for filename in files:\n",
    "                    if filename.endswith(\".txt\"):\n",
    "                        file_path = os.path.join(root, filename)\n",
    "                        content = read_txt(file_path)\n",
    "                        topic = classify_document(content, topics)\n",
    "                        sanitized_topic = sanitize_folder_name(topic)\n",
    "                        topic_folder = os.path.join(doc_folder, sanitized_topic)\n",
    "                        if not os.path.exists(topic_folder):\n",
    "                            os.makedirs(topic_folder)\n",
    "                        os.rename(file_path, os.path.join(topic_folder, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7800dfe6-402f-4988-87e1-4af2e408cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования\n",
    "doc_folder = r\"C:\\Users\\User\\Documents\\data_full\" #папка с документами для распределения\n",
    "docx_file = r\"C:\\Users\\User\\Documents\\docs_from_load\\2_5463026898172925745.docx\" #файл docx с правилами распределения\n",
    "distribute_documents(doc_folder, docx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011ac02-0c83-4c89-a5b4-a3a4f723cf29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
